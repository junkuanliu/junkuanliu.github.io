# VLM + RL + Data (Environment) = GUI Agent

Since GPT-3, teaching AI to operate computers and phones has been a persistent ambition. Many systems have been proposed—some tool-centric, some rule-driven, some fully end-to-end. Given the breadth of prior work, in this post, I focus on one route that is increasingly convergent in the literature: use **reinforcement learning (RL)** to train a **vision-language model (VLM)** to act as a GUI agent end-to-end.

![Screenshot 2025-09-18 at 9.56.22 PM.png](VLM%20+%20RL%20+%20Data%20(Environment)%20=%20GUI%20Agent%20268632381ae280d0b440de9faf19ae57/Screenshot_2025-09-18_at_9.56.22_PM.png)

# Vision-Language Model

A straightforward way to automate GUI action is to use prompt engineering with a VLM and, in multi-stage designs, delegate planning to one model and grounding to another [1–2]. The field is gradually converging on unified architectures where a single vision-language model carries perception, reasoning, and action, motivated by the observation that hand-crafted pipelines tend to saturate while learned systems keep improving with scale and interaction signal; reading “The Bitter Lesson” provides useful historical intuition for why learned, general systems win out over manual decomposition in the long run [3].

General-purpose VLMs bring strong priors from vast pre-training corpora, but GUI control is a distinct domain. Screens are high-resolution and densely populated with small, function-bearing elements; models often miss tiny icons without special treatment [4–5]. Semantics depend on hierarchical layout and affordances rather than natural-image statistics, and robust localization remains hard under resolution constraints [6–7]. GUI interfaces also evolve at runtime: pop-ups, notifications, and layout drift introduce non-stationarity and noise that perception must handle gracefully [8–10]. Finally, tasks unfold over long horizons with stateful dependencies across applications; scalable, reliable long-horizon reasoning remains open [11].

In response to these domain-specific challenges, the community has built GUI-centric perception datasets and models and then used supervised fine-tuning (SFT) to specialize VLMs for precise grounding and action mapping. High-resolution and small-box localization are explicitly targeted by AGuvis-1 and ScreenPR/ScreenSpot-style resources [4, 12], while structural understanding of layout/affordances is stressed by WebSRC [6]. At the same time, OS-ATLAS curates a cross-platform atlas of OS-level elements and tasks and releases baseline models trained via SFT to improve grounded understanding across desktop domains [7]. Concretely, the OS-ATLAS project reports consistent gains of its SFT models on its own cross-app evaluations and supplies reproducible training/evaluation recipes (see project page: [https://osatlas.github.io/](https://osatlas.github.io/)) [7], illustrating the typical “pretrain for perception → SFT for domain specialization” pipeline before moving to RL for robustness under drift and long horizons.

![Screenshot 2025-09-18 at 9.51.17 PM.png](VLM%20+%20RL%20+%20Data%20(Environment)%20=%20GUI%20Agent%20268632381ae280d0b440de9faf19ae57/Screenshot_2025-09-18_at_9.51.17_PM.png)

However, the table above makes the limitation of SFT clear: even with domain-specialized pretraining and supervised fine-tuning, models remain far from human reliability on multi-application desktop workflows. With the rise of verifiable-reward RL at the start of this year—i.e., training regimes that compute rule-based task signals from the environment—RL is increasingly applied to GUI agents to optimize behavior over trajectories rather than single steps [12].

# Reinforcement Learning

RL has a long history in sequential decision-making and. When training GUI Agent, the interaction process between GUI agents and their environments is naturally formalized as a Markov Decision Process (MDP), denoted as M = {S, A, T, r, γ} . In this formulation, the state space S corresponds to possible screen observations; the action space A includes clicks, text entry, scrolling, or higher-level API calls; the transition function T models how the interface evolves; and the reward function r provides scalar feedback. The objective is to learn a policy π(a∣s) that maximizes expected return. This framing makes reinforcement learning a natural fit: unlike supervised fine-tuning, which depends on annotated trajectories, RL can directly optimize behavior through interaction, particularly in long-horizon, multi-application workflows. [13–14].

![Screenshot 2025-09-18 at 10.03.01 PM.png](VLM%20+%20RL%20+%20Data%20(Environment)%20=%20GUI%20Agent%20268632381ae280d0b440de9faf19ae57/Screenshot_2025-09-18_at_10.03.01_PM.png)

People have already begun to apply RL to GUI agents, and the initial results are encouraging. For instance, the UI-TARS and UI-TARS2 systems integrate multimodal perception with trajectory-level reinforcement signals to improve robustness under UI drift and long-horizon tasks, demonstrating clear gains over supervised approaches in mobile and Android environments [14, 15]. Similarly, benchmarks such as OSWorld show that RL-trained policies can generalize better than SFT alone, achieving higher success rates on multi-step workflows across applications [11]. These results suggest that interaction-driven optimization is beginning to narrow the gap toward human-like reliability.

The benefits of RL in this setting are straightforward. GUI tasks incur compounding error and delayed credit assignment; RL directly optimizes long-term return rather than step-wise imitation. Rule-based or “verifiable” rewards allow programmatic checks of progress and completion, reducing the dependence on expensive human labels and potential reward hacking, while offline replay and asynchronous interaction pipelines improve sample efficiency [14]. Empirically, these ingredients translate into better adaptivity under layout drift, more reliable error recovery, and improved cross-application transfer relative to SFT-only baselines.

At the same time, the main obstacles exposed by these RL studies point beyond the optimizer. High-quality, up-to-date trajectories and learning-grade environments are scarce; many public corpora exhibit noise, outdated UI states, or narrow coverage [14, 13]. Environment ecosystems typically fall into three families—static replicas (e.g., /screenshot snapshots), simulated/self-hosted websites (e.g., MiniWoB, WebArena/VisualWebArena), and real-world execution platforms (e.g., OSWorld, AndroidWorld/MobileAgentBench)—each trading off fidelity, controllability, and cost [13, 9–11, 16–18]. **However, none of them yet meet the bar for scalable, verifiable, and economical data generation at once**—each compromises fidelity, controllability, or cost [13, 9–11, 16–18]. In short, as RL moves from proofs-of-concept to realistic settings, data and environments—not just algorithms—will increasingly determine progress.

# Data in RL

When applying RL to GUI agents, it is tempting to keep tuning the optimizer or add intricate reward shaping. But a careful look at failure cases shows that many errors stem from *signal quality* rather than the policy update itself: mislabeled or ambiguous success checks, stale or drifting screens, and missing intermediate feedback. This pattern is consistent with the surveys’ diagnosis that, after reasonable algorithmic baselines, the dominant bottlenecks are noisy/sparse rewards and dataset staleness in rapidly evolving interfaces. In other words, the algorithm may be “good enough” on clean loops; what holds back reliability is the quality and timeliness of the interaction data.
On the environment side, today’s ecosystems span static replicas, simulated/self-hosted sites, and real-world execution platforms, each trading off fidelity, controllability, and cost [13]. **Beyond environments, however, there is still no generally accepted, scalable method to acquire *large-scale, high-quality* trajectory data for RL.** Existing collection pipelines either introduce too much noise (e.g., weakly verified outcomes, outdated UI states, inconsistent progress signals) or fail to scale economically to the volumes needed for robust long-horizon learning. In practice, this means policies trained with interaction remain constrained not only by algorithmic choices but by the *lack of reliable, progress-aware trajectories* at scale.

**From a data perspective, the most promising path I see is to *engineer environments as data generators*, not because environments are the only solution, but because they can operationalize what high-quality trajectories require.** If an environment can (i) **verify** terminal and intermediate conditions programmatically (reducing label noise), (ii) **control** drift and perturbations to expose robustness gaps systematically, and (iii) **scale** via parallel, replayable execution, then it becomes a practical instrument for producing large volumes of reliable, progress-aware interaction data—precisely the bottleneck identified above. This does not preclude complementary routes (e.g., policy-guided logging with consent, rule-based reward extraction, or offline value modeling), but it frames a unifying principle: treat data quality as a *system property* that we can design for. In this view, environments are not an end in themselves; they are a means to manufacture the signals that RL needs to turn SFT competence into reliable, long-horizon behavior in dynamic GUIs.

# References

[1] OmniParser. “Structured UI Representations from Screenshots for GUI Grounding.” arXiv preprint, 2024.
[2] SeeClick. “SeeClick: Harnessing General-Purpose Models for Click-Through GUI Grounding.” Findings of ACL, 2024.
[3] Sutton, R. S. “The Bitter Lesson.” 2019. [https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf)
[4] ScreenSpot-Pro. “GUI Grounding for Professional High-Resolution Computer Use.” arXiv preprint, 2025.
[5] CogAgent. “CogAgent: A Visual–Language Model for GUI Perception and Small-Element Localization.” arXiv preprint / Conference paper, 2024.
[6] WebSRC. “Web-based Structural Reading Comprehension.” EMNLP, 2021. [https://arxiv.org/abs/2107.04612](https://arxiv.org/abs/2107.04612)
[7] OS-ATLAS. “A Cross-Platform Operating-System GUI Atlas for Grounded Understanding.” Project & preprint, 2024. [https://osatlas.github.io/](https://osatlas.github.io/)
[8] UI-Hawk. “History-Aware GUI Agents under Dynamic Interfaces.” preprint, 2025.
[9] WebArena. “A Realistic Web Environment for Building Autonomous Agents.” Project & arXiv preprint, 2023. [https://arxiv.org/abs/2307.13854](https://arxiv.org/abs/2307.13854) — [https://webarena.dev](https://webarena.dev/)
[10] VisualWebArena. “Evaluating Multimodal Agents on Realistic Visual Web Tasks.” arXiv preprint / Benchmark project, 2024.
[11] OSWorld. “Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments.” NeurIPS Datasets & Benchmarks / arXiv preprint, 2024. [https://arxiv.org/abs/2404.07972](https://arxiv.org/abs/2404.07972) — [https://os-world.github.io](https://os-world.github.io/)
[12] DeepSeek-R1. “Incentivizing Reasoning via Verifiable Rewards.” arXiv preprint, 2025. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
[13] Survey (Perception–Exploration–Planning–Interaction). “A Survey on (M)LLM-Based GUI Agents.” arXiv preprint, 2025 (v1: 2504.13865).
[14] Survey (RL-Enhanced GUI Agents). “A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning.” arXiv preprint, 2025 (v2: 2504.20464).
[15] UI-TARS / UI-TARS2. “Unified Perception-to-Action Mobile/Android Agents with Reinforcement Optimization.” project & papers, 2024–2025.
[16] MiniWoB / MiniWoB++. “World of Bits: An Open-Domain Platform for Web-Based Agents (incl. MiniWoB).” ICML, 2017. [https://arxiv.org/abs/1707.06690](https://arxiv.org/abs/1707.06690)
[17] AndroidWorld. “A Dynamic Real-Device Benchmarking Environment for Autonomous Agents.” arXiv preprint, 2024. [https://arxiv.org/abs/2405.14573](https://arxiv.org/abs/2405.14573)
[18] MobileAgentBench. “A Benchmark for Mobile LLM Agents on Real Devices.” arXiv preprint, 2024. [https://arxiv.org/abs/2406.08184](https://arxiv.org/abs/2406.08184)